{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emoji.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay-rsC3guqVe",
        "colab_type": "code",
        "outputId": "42efa57d-161f-4057-9e61-d0bb9e45d677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWP4DgaTu-vd",
        "colab_type": "code",
        "outputId": "f8ccbeb6-64e5-4982-fdd3-0c711bf7edff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        }
      },
      "source": [
        "pip install easybert"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting easybert\n",
            "  Downloading https://files.pythonhosted.org/packages/b1/0c/783b52c939a0b9faa3d4fca152343fb899bbefa9afa388d14d389db019db/easybert-1.0.3.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from easybert) (1.17.5)\n",
            "Collecting tensorflow-hub==0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/5c/6f3698513cf1cd730a5ea66aec665d213adf9de59b34f362f270e0bd126f/tensorflow_hub-0.4.0-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 2.7MB/s \n",
            "\u001b[?25hCollecting bert-tensorflow==1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from easybert) (7.0)\n",
            "Collecting tensorflow-gpu==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
            "\u001b[K     |████████████████████████████████| 345.2MB 46kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub==0.4.0->easybert) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub==0.4.0->easybert) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->easybert) (0.34.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->easybert) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->easybert) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->easybert) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->easybert) (0.9.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 33.2MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 27.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->easybert) (0.2.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->easybert) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->easybert) (1.27.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub==0.4.0->easybert) (45.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1->easybert) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1->easybert) (3.2.1)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/30/6a/9bde648117ec7087c89a45de0a8b25aba21d54d3defd08cb24eacded875f/mock-4.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1->easybert) (2.8.0)\n",
            "Building wheels for collected packages: easybert\n",
            "  Building wheel for easybert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for easybert: filename=easybert-1.0.3-cp36-none-any.whl size=7621 sha256=9700f4a553ec12c0aaf5021505bbbf1c2ba1add3d2292123ce3bc7b1bc376e0c\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/00/9f/587ad878f58cb7255674e9027c546893e8a921b13cec954510\n",
            "Successfully built easybert\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.13.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-hub, bert-tensorflow, tensorboard, mock, tensorflow-estimator, tensorflow-gpu, easybert\n",
            "  Found existing installation: tensorflow-hub 0.7.0\n",
            "    Uninstalling tensorflow-hub-0.7.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.7.0\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed bert-tensorflow-1.0.1 easybert-1.0.3 mock-4.0.1 tensorboard-1.13.1 tensorflow-estimator-1.13.0 tensorflow-gpu-1.13.1 tensorflow-hub-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSpZqUrB-zhH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f1091383-a311-488b-beac-7c4b04c17175"
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qAGaS9UvKsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import nltk\n",
        "import re\n",
        "import sklearn\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49W1qHWVv84U",
        "colab_type": "code",
        "outputId": "c94a3e6b-7946-40e3-d981-d3477d83d26b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "from easybert import Bert\n",
        "bert = Bert(\"https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW-NHcz9w85O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing datasets#\n",
        "#Train set\n",
        "file1 = open(\"/content/drive/My Drive/Emoji_prediction/us_train.text\", \"r+\", encoding=\"utf-8\")\n",
        "train_text = file1.readlines()\n",
        "file2 = open(\"/content/drive/My Drive/Emoji_prediction/us_train.labels\", \"r+\", encoding=\"utf-8\")\n",
        "train_labels = file2.readlines()\n",
        "#Development set\n",
        "file3 = open(\"/content/drive/My Drive/Emoji_prediction/us_dev.text\", \"r+\", encoding=\"utf-8\")\n",
        "dev_text = file3.readlines()\n",
        "file4 = open(\"/content/drive/My Drive/Emoji_prediction/us_dev.labels\", \"r+\", encoding=\"utf-8\")\n",
        "dev_labels = file4.readlines()\n",
        "#Test set\n",
        "file5 = open(\"/content/drive/My Drive/Emoji_prediction/us_test.text\", \"r+\", encoding=\"utf-8\")\n",
        "test_text = file5.readlines()\n",
        "file6 = open(\"/content/drive/My Drive/Emoji_prediction/us_test.labels\", \"r+\", encoding=\"utf-8\")\n",
        "test_labels = file6.readlines()\n",
        "\n",
        "\n",
        "\n",
        "train_set = []\n",
        "dev_set = []\n",
        "test_set = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MUzhr_qCM9x",
        "colab_type": "text"
      },
      "source": [
        "**Text Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwvXHsbH3s-h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f70af4ac-9a2f-4aaf-fcac-afec224c8729"
      },
      "source": [
        "#Initalizing lemmatizer and creating a list of stopwords\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "\n",
        "#Cleans reviews, removes stopwords and lemmatize to put the reviews in a form that machine can perform better\n",
        "def clean_reviews(string):\n",
        "    lemmatized_string = \"\"\n",
        "    #Remove HTML information from the string\n",
        "    cleaner = re.compile('<.*?>')\n",
        "    processed_string = re.sub(cleaner,\" \", string) \n",
        "    \n",
        "    #Remove URLS from the string\n",
        "    processed_string = re.compile(r\"https?://[A-Za-z0-9./]+\").sub(\" \", processed_string)\n",
        "    \n",
        "    #Remove digits and punctuations\n",
        "    processed_string = re.compile(r\"[^a-zA-Z ]\").sub(\" \", processed_string)\n",
        "    \n",
        "    #Lowercase all the words\n",
        "    processed_string = processed_string.lower()\n",
        "    \n",
        "    #Does not append stopwords but appends other strings while lemmatizing them\n",
        "    for word in processed_string.split():\n",
        "        if word in stopwords:\n",
        "            continue\n",
        "        else:\n",
        "            lemmatized_string += lemmatizer.lemmatize(word) + \" \"\n",
        "    return lemmatized_string\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ0dBt8_4MZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train, Dev, Test sets of after data preprocessing#\n",
        "train_processed_text = []\n",
        "train_processed_labels = []\n",
        "dev_processed_text = []\n",
        "dev_processed_labels = []\n",
        "test_processed_text = []\n",
        "test_processed_labels = []\n",
        "\n",
        "\n",
        "for i in train_text:\n",
        "  train_processed_text.append(clean_reviews(i))\n",
        "for i in dev_text:\n",
        "  dev_processed_text.append(clean_reviews(i))\n",
        "for i in test_text:\n",
        "  test_processed_text.append(clean_reviews(i))\n",
        "for i in train_labels:\n",
        "   train_processed_labels.append(i.replace(\"\\n\", \"\"))\n",
        "for i in dev_labels:\n",
        "   dev_processed_labels.append(i.replace(\"\\n\", \"\"))\n",
        "for i in test_labels:\n",
        "   test_processed_labels.append(i.replace(\"\\n\", \"\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pquk48Rf3pM_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11fabb37-3610-42df-c5e4-400b30f0bfe0"
      },
      "source": [
        "print(train_processed_text[:10])\n",
        "print(train_processed_labels[0:10])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['12', '19', '0', '0', '2', '11', '0', '19', '0', '7']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqSYLNmMB4bv",
        "colab_type": "text"
      },
      "source": [
        "**Feature** **extraction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyG0rcB2B9Bh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_vector = []\n",
        "test_vector = []\n",
        "for i in range(5000):\n",
        "  train_vector.append(bert.embed(train_processed_text[i]))\n",
        "for i in range(5000):\n",
        "  test_vector.append(bert.embed(test_processed_text[i]))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4ZSI7yMDNHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_vector)\n",
        "print(train_processed_labels[0:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnQX3TM9Dk3g",
        "colab_type": "text"
      },
      "source": [
        "**Model Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0V2LpxjMdDV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d72d6b06-1f4a-4440-dd00-fc4822a5387f"
      },
      "source": [
        "#Train random forest model#\n",
        "rfc = RandomForestClassifier()\n",
        "rfc.fit(train_vector, train_processed_labels[0:10])\n",
        "display(rfc.score(train_vector, train_processed_labels[0:10]))\n",
        "\n",
        "#Predict the test_set\n",
        "y_predict = rfc.predict(test_vector)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehbDgmFGPpEr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "964acca6-8e8a-4c4b-ff04-3135cf3662c3"
      },
      "source": [
        "#Calculate precision, recall, f1 score and accuracy of the model\n",
        "precision = precision_score(test_processed_labels, y_predict, average='macro')\n",
        "recall = recall_score(test_processed_labels, y_predict, average='macro')\n",
        "f1 = f1_score(test_processed_labels, y_predict, average='macro')\n",
        "accuracy = accuracy_score(test_processed_labels, y_predict)\n",
        "\n",
        "#Print the evaluation measures\n",
        "print (\"Precision: \"+str(round(precision,5)))\n",
        "print (\"Recall: \"+str(round(recall,5)))\n",
        "print (\"F1-Score: \"+str(round(f1,5)))\n",
        "print (\"Accuracy: \"+str(round(accuracy,5)))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['0' '19' '0' '0' '0' '0' '0' '19' '11' '0']\n",
            "['2', '10', '6', '1', '16', '17', '4', '10', '12', '18']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}